{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trannhan/WorkSpace/NPL_LSTM/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "\n",
    "# from google.colab import files\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "  def __init__(self, hidden_dim):\n",
    "    super(LuongAttention, self).__init__()\n",
    "\n",
    "    self.w = layers.Dense(hidden_dim, name='encoder_outputs_dense')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    encoder_output_seq, decoder_output = inputs\n",
    "    z = self.w(encoder_output_seq)\n",
    "    attention_scores = tf.matmul(decoder_output, z, transpose_b=True)\n",
    "    attention_weights = tf.keras.activations.softmax(attention_scores, axis=-1)\n",
    "    context = tf.matmul(attention_weights, encoder_output_seq)\n",
    "\n",
    "    return attention_weights, context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # No masking here. We'll handle it ourselves.\n",
    "        self.embedding = layers.Embedding(vocab_size,\n",
    "                                          embedding_dim,\n",
    "                                          name='encoder_embedding_layer')\n",
    "\n",
    "        # return_sequences is set to True this time.\n",
    "        self.lstm = layers.LSTM(hidden_dim,\n",
    "                                return_sequences=True,\n",
    "                                return_state=True,\n",
    "                                name='encoder_lstm')\n",
    "\n",
    "    def call(self, input):\n",
    "        embeddings = self.embedding(input)\n",
    "\n",
    "        # output_seq will hold the encoder's hidden states from each time step.\n",
    "        output_seq, state_h, state_c = self.lstm(embeddings)\n",
    "\n",
    "        return output_seq, state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.embedding_layer = layers.Embedding(vocab_size,\n",
    "                                            embedding_dim,\n",
    "                                            name='decoder_embedding_layer')\n",
    "\n",
    "    self.lstm = layers.LSTM(hidden_dim,\n",
    "                            return_sequences=True,\n",
    "                            return_state=True,\n",
    "                            name='decoder_lstm')\n",
    "\n",
    "    self.attention = LuongAttention(hidden_dim)\n",
    "\n",
    "    self.w = tf.keras.layers.Dense(hidden_dim, activation='tanh', name='attended_outputs_dense')\n",
    "\n",
    "    self.dense = layers.Dense(vocab_size, name='decoder_dense')\n",
    "\n",
    "\n",
    "  def call(self, inputs):\n",
    "    decoder_input, encoder_output_seq, lstm_state = inputs\n",
    "    embeddings = self.embedding_layer(decoder_input)\n",
    "\n",
    "    decoder_output, state_h, state_c = self.lstm(embeddings, initial_state=lstm_state)\n",
    "\n",
    "    weights, context = self.attention([encoder_output_seq, decoder_output])\n",
    "\n",
    "    decoder_output_with_attention = self.w(tf.concat(\n",
    "        [tf.squeeze(context, 1), tf.squeeze(decoder_output, 1)], -1))\n",
    "\n",
    "    logits = self.dense(decoder_output_with_attention)\n",
    "\n",
    "    return logits, state_h, state_c, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/tokenizer/source_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    source_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "with open('./models/tokenizer/target_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    target_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_vocab_size:  20814\n",
      "target_vocab_size:  7088\n"
     ]
    }
   ],
   "source": [
    "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "print(\"source_vocab_size: \", source_vocab_size)\n",
    "print(\"target_vocab_size: \", target_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "default_dropout=0.2\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(34, embedding_dim, hidden_dim)\n",
    "encoder = Encoder(43, embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x169631d00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.load_weights(\"./models/luong_attention_weights_new/attention_decoder_weights_with_dropout_ckpt\")\n",
    "encoder.load_weights(\"./models/luong_attention_weights_new/attention_encoder_weights_with_dropout_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_encoding_len = 48\n",
    "def translate_with_attention(sentence: str,\n",
    "                             source_tokenizer, encoder,\n",
    "                             target_tokenizer, decoder,\n",
    "                             max_translated_len = 50):\n",
    "    input_seq = source_tokenizer.texts_to_sequences([sentence])\n",
    "    tokenized = source_tokenizer.sequences_to_texts(input_seq)\n",
    "\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_encoding_len, padding='post')\n",
    "    encoder_output, state_h, state_c  = encoder.predict(input_seq)\n",
    "\n",
    "    current_word = '<sos>'\n",
    "    decoded_sentence = []\n",
    "\n",
    "    while len(decoded_sentence) < max_translated_len:\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = target_tokenizer.word_index[current_word]\n",
    "\n",
    "        logits, state_h, state_c, _ = decoder.predict([target_seq, encoder_output, (state_h, state_c)])\n",
    "        current_token_index = np.argmax(logits[0])\n",
    "\n",
    "        current_word = target_tokenizer.index_word[current_token_index]\n",
    "\n",
    "        if (current_word == '<eos>'):\n",
    "          break\n",
    "\n",
    "        decoded_sentence.append(current_word)\n",
    "\n",
    "    return tokenized[0], ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(s):\n",
    "    #s = normalize_unicode(s)\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    s = s.strip()\n",
    "    s = s.lower()  # Chuyển đổi thành chữ thường\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentences_2(test_input, translation_func, source_tokenizer, encoder,\n",
    "                        target_tokenizer, decoder, max_translated_len):\n",
    "    test_input = preprocess_sentence(test_input)\n",
    "    tokenized_sentence, translated = translation_func(test_input, source_tokenizer, encoder,\n",
    "                                                      target_tokenizer, decoder, max_translated_len)\n",
    "    print(tokenized_sentence)\n",
    "    print(translated)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 347ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1715589142.253644       1 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "the patient may pass away at any moment .\n",
      "bệnh nhân có thể vượt qua bất cứ lúc nào .\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Honestly, I never expected this collaboration.\"\n",
    "test_2 = \"Tom took a book from the shelf.\"\n",
    "test_3 = \"Tom makes great cookies.\"\n",
    "test_4 = \"I didn't care if i won or not.\"\n",
    "test_5 = \"How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\"\n",
    "test_6 = \"We’re going to make sure that no one is taking advantage of the American people for their own short-term gain. \"\n",
    "translate_sentences_2(\"the patient may pass away at any moment .\", translate_with_attention, source_tokenizer, encoder,\n",
    "                                                             target_tokenizer, decoder, max_translated_len=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
