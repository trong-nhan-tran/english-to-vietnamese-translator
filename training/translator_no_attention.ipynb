{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trannhan/WorkSpace/NPL_LSTM/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "import unicodedata\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loaf dataset\n",
    "with open('./dataset/en_sents.txt') as file:\n",
    "  train_input = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/vi_sents.txt') as file:\n",
    "  train_target = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please put the dustpan in the broom closet', 'Be quiet for a moment.', 'Read this'] Length:  254090\n",
      "['xin vui lòng đặt người quét rác trong tủ chổi', 'im lặng một lát', 'đọc này'] Length:  254090\n"
     ]
    }
   ],
   "source": [
    "print(train_input[:3], \"Length: \", len(train_input))\n",
    "print(train_target[:3], \"Length: \", len(train_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 00:46:52.870462: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:52.877105: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:52.920095: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.146672: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.155230: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.160610: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.547368: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.553006: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.557839: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.714454: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.719207: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.802198: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.807157: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.862251: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.954754: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.959493: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.964588: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:53.969769: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:54.004177: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "2024-05-09 00:46:54.009236: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "model = tf.keras.models.load_model('./models/eng_vie_no_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"eng_vi_seq2seq_nmt_no_attention\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_embeddings (Embedd  (None, None, 128)            2664192   ['encoder_inputs[0][0]']      \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " decoder_embeddings (Embedd  (None, None, 128)            907264    ['decoder_inputs[0][0]']      \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)         [(None, 256),                394240    ['encoder_embeddings[0][0]']  \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)         [(None, None, 256),          394240    ['decoder_embeddings[0][0]',  \n",
      "                              (None, 256),                           'encoder_lstm[0][1]',        \n",
      "                              (None, 256)]                           'encoder_lstm[0][2]']        \n",
      "                                                                                                  \n",
      " decoder_dense (Dense)       (None, None, 7088)           1821616   ['decoder_lstm[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6181552 (23.58 MB)\n",
      "Trainable params: 6181552 (23.58 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "default_dropout=0.2\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/tokenizer/source_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    source_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "with open('./models/tokenizer/target_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    target_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get layer encoder\n",
    "encoder_inputs = model.get_layer('encoder_inputs').input\n",
    "\n",
    "encoder_embedding_layer = model.get_layer('encoder_embeddings')\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_inputs)\n",
    "\n",
    "encoder_lstm = model.get_layer('encoder_lstm')\n",
    "\n",
    "_, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embeddings)\n",
    "\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "# Our stand-alone encoder model. encoder_inputs is the input to the encoder,\n",
    "# and encoder_states is the expected output.\n",
    "encoder_model_no_attention = tf.keras.Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get layer decoder\n",
    "decoder_inputs = model.get_layer('decoder_inputs').input\n",
    "\n",
    "decoder_embedding_layer = model.get_layer('decoder_embeddings')\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "# Inputs to represent the decoder's LSTM hidden and cell states. We'll populate\n",
    "# these manually using the encoder's output for the initial state.\n",
    "decoder_input_state_h = tf.keras.Input(shape=(hidden_dim,), name='decoder_input_state_h')\n",
    "decoder_input_state_c = tf.keras.Input(shape=(hidden_dim,), name='decoder_input_state_c')\n",
    "decoder_input_states = [decoder_input_state_h, decoder_input_state_c]\n",
    "\n",
    "decoder_lstm = model.get_layer('decoder_lstm')\n",
    "\n",
    "decoder_sequence_outputs, decoder_output_state_h, decoder_output_state_c = decoder_lstm(\n",
    "    decoder_embeddings, initial_state=decoder_input_states\n",
    ")\n",
    "\n",
    "# Update hidden and cell states for the next time step.\n",
    "decoder_output_states = [decoder_output_state_h, decoder_output_state_c]\n",
    "\n",
    "decoder_dense = model.get_layer('decoder_dense')\n",
    "y_proba = decoder_dense(decoder_sequence_outputs)\n",
    "\n",
    "decoder_model_no_attention = tf.keras.Model(\n",
    "    [decoder_inputs] + decoder_input_states,\n",
    "    [y_proba] + decoder_output_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_without_attention(sentence: str,\n",
    "                                source_tokenizer, encoder,\n",
    "                                target_tokenizer, decoder,\n",
    "                                max_translated_len = 30):\n",
    "\n",
    "  # Vectorize the source sentence and run it through the encoder.\n",
    "  input_seq = source_tokenizer.texts_to_sequences([sentence])\n",
    "\n",
    "  # Get the tokenized sentence to see if there are any unknown tokens.\n",
    "  tokenized_sentence = source_tokenizer.sequences_to_texts(input_seq)\n",
    "\n",
    "  states = encoder.predict(input_seq)\n",
    "\n",
    "  current_word = '<sos>'\n",
    "  decoded_sentence = []\n",
    "\n",
    "  while len(decoded_sentence) < max_translated_len:\n",
    "\n",
    "    # Set the next input word for the decoder.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = target_tokenizer.word_index[current_word]\n",
    "\n",
    "    # Determine the next word.\n",
    "    target_y_proba, h, c = decoder.predict([target_seq] + states)\n",
    "    target_token_index = np.argmax(target_y_proba[0, -1, :])\n",
    "    current_word = target_tokenizer.index_word[target_token_index]\n",
    "\n",
    "    if (current_word == '<eos>'):\n",
    "      break\n",
    "\n",
    "    decoded_sentence.append(current_word)\n",
    "    states = [h, c]\n",
    "\n",
    "  return tokenized_sentence[0], ' '.join(decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(s):\n",
    "    #s = normalize_unicode(s)\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    s = s.strip()\n",
    "    s = s.lower()  # Chuyển đổi thành chữ thường\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentences(test_input, translation_func, source_tokenizer, encoder,\n",
    "                        target_tokenizer, decoder):\n",
    "    test_input = preprocess_sentence(test_input)\n",
    "    tokenized_sentence, translated = translation_func(test_input, source_tokenizer, encoder,\n",
    "                                                      target_tokenizer, decoder)\n",
    "    print(tokenized_sentence)\n",
    "    print(translated)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "the patient may pass away at any moment .\n",
      "bệnh nhân có thể vượt qua bất cứ lúc nào .\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"she said she played soccer yesterday.\"\n",
    "test_3 = \"Tom makes great cookies.\"\n",
    "\n",
    "translate_sentences(\"the patient may pass away at any moment .\", translate_without_attention, source_tokenizer, encoder_model_no_attention,\n",
    "                                                             target_tokenizer, decoder_model_no_attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
